<DevSystem MarkdownTablesAllowed=true />

# STRUT: Transcription Model Cost/Quality Evaluation

**Doc ID**: TRNGFX-ST01
**Goal**: Find optimal cost/quality tradeoff for ASCII art transcription models
**Budget**: $20 max
**Approach**: Evaluate 8 transcription models on 5 diverse images, 3 runs each
**Status**: COMPLETE

## MUST-NOT-FORGET

- Use existing transcriptions (already paid for) - do NOT re-transcribe
- 3 runs minimum per image to measure variability
- GPT-5-mini as answerer (cheapest quality option)
- GPT-5 as judge ($1.25 input, $10 output per 1M)
- Save results incrementally, not at end (TRNGFX-FL-002/004)
- Final goal: select best transcription out of 3 if page contains images

## Test Configuration

**Images selected** (5 diverse types):
1. `Edison-Financial-Report-2023-FY2023-Results_page004` - Financial charts/tables
2. `SSE-Annual-Report-2024-Group-Risk-Report_page002` - Risk matrix diagram
3. `Werner_2016_Stem_Cell_Networks_page014` - Scientific diagram
4. `Microsoft-365-Copilot-Adotion-Playbook_page003` - Business infographic
5. `Rent_Prices_Around_the_World_SITE-2` - Data visualization map

**Transcription models** (8 total):
1. `Claude_Opus_4` - Premium baseline ($0.111/image)
2. `Claude_Haiku_3_5` - Budget option ($0.004/image)
3. `Claude_Sonnet_4` - Mid-tier ($0.024/image)
4. `GPT-5-mini` - OpenAI mid-tier ($0.017/image)
5. `GPT_5` - OpenAI premium ($0.100/image)
6. `GPT_4_1` - OpenAI premium ($0.045/image)
7. `GPT_4o` - OpenAI mid-tier ($0.038/image)
8. `GPT_5_2` - OpenAI newest ($0.047/image)

**Runs**: 3 per image (run01, run02, run03)
**Total transcriptions**: 5 images x 8 models x 3 runs = 120 files

## Cost Estimate

**Answering** (GPT-5-mini @ $0.25 input, $2 output per 1M):
- 60 transcriptions x 10 questions = 600 answers
- ~3K tokens per answer cycle (transcription + question + answer)
- Total: ~1.8M tokens = ~$4

**Judging** (GPT-5 @ $1.25 input, $10 output per 1M):
- 600 items x ~500 tokens = 300K tokens
- Total: ~$3.50

**Estimated total**: ~$8-10 (within $20 budget)

## Phase 1: SETUP
- [x] P1-S1: Create folder structure
- [x] P1-S2: Copy transcriptions from existing outputs (3 runs only)
- [x] P1-S3: Extract subset of evaluation questions for 5 images
- [x] P1-D1: Ready for answer generation (60 files, 50 questions)

## Phase 2: ANSWER
- [x] P2-S1: Generate answers for Claude_Opus_4 transcriptions
- [x] P2-S2: Generate answers for Claude_Haiku_3_5 transcriptions
- [x] P2-S3: Generate answers for Claude_Sonnet_4 transcriptions
- [x] P2-S4: Generate answers for GPT-5-mini transcriptions
- [x] P2-S5: Generate answers for GPT_5 transcriptions
- [x] P2-S6: Generate answers for GPT_4_1 transcriptions
- [x] P2-S7: Generate answers for GPT_4o transcriptions
- [x] P2-S8: Generate answers for GPT_5_2 transcriptions
- [x] P2-D1: All answer files created (400 answers total)

## Phase 3: JUDGE
- [x] P3-S1: Score all 8 models using GPT-5
- [x] P3-D1: All score files created (400 scores)

## Phase 4: ANALYZE
- [x] P4-S1: Calculate average scores per model
- [x] P4-S2: Calculate scores by category
- [x] P4-S3: Calculate cost per quality point
- [x] P4-D1: Analysis complete (analysis_results.json)

## Phase 5: CONCLUDE
- [x] P5-S1: Identify best cost/quality model -> GPT-5-mini
- [x] P5-S2: Write recommendation to PROBLEMS.md
- [x] P5-S3: Write INFO report (INFO_ASCII_ART_TRANSCRIPTION_COST_QUALITY_EVAL.md)
- [x] P5-S4: Verify findings using /verify workflow
- [x] P5-D1: Evaluation complete

**Transition**: P1 -> P2 -> P3 -> P4 -> P5 -> [END] âœ“

## Results Summary (8 Models)

| Model | Quality | Pass | Cost/Image | Value |
|-------|---------|------|------------|-------|
| Claude Opus 4 | 3.56/5 | 66% | $0.111 | 32 pts/$ |
| GPT-5.2 | 3.56/5 | 66% | $0.047 | 76 pts/$ |
| **GPT-5-mini** | **3.48/5** | **62%** | **$0.017** | **205 pts/$** |
| GPT-5 | 3.38/5 | 64% | $0.100 | 34 pts/$ |
| GPT-4.1 | 3.36/5 | 62% | $0.045 | 75 pts/$ |
| Claude Sonnet 4 | 3.08/5 | 52% | $0.024 | 128 pts/$ |
| GPT-4o | 2.80/5 | 48% | $0.038 | 74 pts/$ |
| Claude Haiku 3.5 | 2.70/5 | 44% | $0.004 | 675 pts/$ |

**Best Value**: GPT-5-mini - 98% quality at 15% cost (vs Opus 4)
**Best Quality (mid-tier cost)**: GPT-5.2 - ties Opus 4 at 42% cost

**Not recommended**:
- GPT-5: Lower quality than GPT-5-mini at 6x cost
- GPT-4o: Lower quality than GPT-5-mini at 2x cost
- Claude Haiku 3.5: Quality too low for production

## Artifacts Created

- `INFO_ASCII_ART_TRANSCRIPTION_COST_QUALITY_EVAL.md` - Full evaluation report
- `analysis_results.json` - Raw analysis data
- `scores/*/scores_gpt-5.json` - Score files per model
- `answers/*/answers_gpt-5-mini.json` - Answer files per model

## Document History

**[2026-01-23 00:40]**
- Added GPT-5.2 (ties with Opus 4 at 3.56/5)
- Expanded to 8 models total
- Created INFO report with verified findings
- Ran /verify workflow

**[2026-01-23 00:25]**
- All phases complete for initial 4 models
- GPT-5-mini identified as optimal model
- Updated PROBLEMS.md with recommendation

**[2026-01-23 00:10]**
- Initial STRUT created
